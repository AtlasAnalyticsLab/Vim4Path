{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "873c3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from natsort import os_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a7207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'Extracted_labels/Balanced/224_10x/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9365d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dino.vision_transformer import DINOHead, VisionTransformer\n",
    "from dino.vim.models_mamba import VisionMamba\n",
    "from dino.config import configurations\n",
    "from dino.main import get_args_parser\n",
    "from functools import partial\n",
    "from dino.utils import load_pretrained_weights\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d86968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_args_parser()\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5699c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "\n",
    "    config = configurations[args.arch]\n",
    "    config['img_size'] = args.image_size\n",
    "    config['patch_size'] = args.patch_size\n",
    "    config['num_classes'] = args.num_classes\n",
    "    if args.arch in configurations:\n",
    "        config = configurations[args.arch]\n",
    "        config['img_size'] = args.image_size\n",
    "        config['patch_size'] = args.patch_size\n",
    "        config['num_classes'] = args.num_classes\n",
    "\n",
    "        if 'norm_layer' in config and config['norm_layer'] == \"nn.LayerNorm\":\n",
    "            config['norm_layer'] = partial(nn.LayerNorm, eps=config['eps'])\n",
    "        config['drop_path_rate'] = 0  \n",
    "        if args.arch.startswith('vim'):\n",
    "            model = VisionMamba(return_features=True, **config)\n",
    "            embed_dim = model.embed_dim\n",
    "        elif args.arch.startswith('vit'):\n",
    "            model = VisionTransformer(**config)\n",
    "            embed_dim = model.embed_dim * (args.n_last_blocks + int(args.avgpool_patchtokens))\n",
    "        print('EMBEDDED DIM:', embed_dim)\n",
    "    else:\n",
    "        print(f\"Unknown architecture: {args.arch}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18898d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDED DIM: 384\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vim-s_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 384\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "EMBEDDED DIM: 1536\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 1536\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/imagenet/dino_deitsmall16_pretrain.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "args.image_size = 224\n",
    "args.patch_size = 16\n",
    "args.num_classes = 2\n",
    "args.n_last_blocks = 4\n",
    "args.avgpool_patchtokens = False\n",
    "\n",
    "args.checkpoint_key = 'teacher'\n",
    "\n",
    "args.arch = 'vim-s'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vim-s_224-96/checkpoint.pth'\n",
    "model_vim = get_model(args)\n",
    "model_vim.cuda()\n",
    "model_vim.eval()\n",
    "load_pretrained_weights(model_vim, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "args.arch = 'vim-s2'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/imagenet/vim_s_midclstok_80p5acc.pth'\n",
    "model_vim2 = get_model(args)\n",
    "state_dict = torch.load(args.pretrained_weights, map_location=\"cpu\")['model']\n",
    "\n",
    "\n",
    "# Adjusting the state dict to skip loading for layers that have a size mismatch\n",
    "for name, param in model_vim2.named_parameters():\n",
    "    if name not in state_dict or state_dict[name].size() != param.size():\n",
    "        print(f\"Skipping loading parameter {name} due to size mismatch or it not being present in the checkpoint.\")\n",
    "        state_dict.pop(name, None)  # Remove incompatible parameters\n",
    "\n",
    "model_vim2.load_state_dict(state_dict, strict=False)\n",
    "model_vim2.cuda()\n",
    "model_vim2.eval()\n",
    "\n",
    "args.arch = 'vit-s'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth'\n",
    "model_vit = get_model(args)\n",
    "model_vit.cuda()\n",
    "model_vit.eval()\n",
    "load_pretrained_weights(model_vit, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "\n",
    "\n",
    "args.arch = 'vit-s'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/imagenet/dino_deitsmall16_pretrain.pth'\n",
    "model_vit2 = get_model(args)\n",
    "model_vit2.cuda()\n",
    "model_vit2.eval()\n",
    "load_pretrained_weights(model_vit2, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97cf1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(args.image_size, interpolation=3),\n",
    "    transforms.CenterCrop(args.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b076d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "325c0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdd6d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform_vit(tensor, height=14, width=14):\n",
    "    result = tensor[:, 1 :  , :].reshape(tensor.size(0),\n",
    "        height, width, tensor.size(2))\n",
    "\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59d37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform_vim(tensor, height=14, width=14, token_position=98):\n",
    "    hidden_state = tensor\n",
    "    hidden_state = torch.cat((hidden_state[:, 1:token_position, :], hidden_state[:, token_position+1:, :]), dim=1)\n",
    "    result = hidden_state.reshape(hidden_state.size(0), height, width, hidden_state.size(2))\n",
    "    result = result.transpose(2, 3).transpose(1, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96604099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c82568e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((384,), eps=1e-06, elementwise_affine=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ec59a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:58<00:00,  1.03it/s]\n",
      "100%|███████████████████████████████████████████| 60/60 [00:58<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for class_name in ['tumor', 'normal']:\n",
    "\n",
    "    img_paths = glob(os.path.join(dataset_dir, class_name, \"*jpg\"))\n",
    "    img_paths = os_sorted(img_paths)\n",
    "\n",
    "    os.makedirs(f'heatmaps/{class_name}', exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(60)):\n",
    "        img = Image.open(img_paths[i])\n",
    "        img_transformed = val_transform(img).unsqueeze(0)\n",
    "        cam_vim = GradCAM(model=model_vim, target_layers=[model_vim.layers[-1].drop_path],\n",
    "                          reshape_transform=reshape_transform_vim)\n",
    "        grayscale_cam_vim = cam_vim(input_tensor=img_transformed)\n",
    "        grayscale_cam_vim = grayscale_cam_vim[0, :]  \n",
    "\n",
    "        \n",
    "        cam_vim_imagenet = GradCAM(model=model_vim2, target_layers=[model_vim2.layers[-1].drop_path],\n",
    "                          reshape_transform=reshape_transform_vim)\n",
    "        grayscale_cam_vim_imagenet = cam_vim_imagenet(input_tensor=img_transformed)\n",
    "        grayscale_cam_vim_imagenet = grayscale_cam_vim_imagenet[0, :]  \n",
    "        \n",
    "        cam_vit = GradCAM(model=model_vit, target_layers=[model_vit.blocks[-1].norm1],\n",
    "                          reshape_transform=reshape_transform_vit)\n",
    "        grayscale_cam_vit = cam_vit(input_tensor=img_transformed)\n",
    "        grayscale_cam_vit = grayscale_cam_vit[0, :]  \n",
    "\n",
    "\n",
    "        cam_vit_imagenet = GradCAM(model=model_vit2, target_layers=[model_vit2.blocks[-1].norm1],\n",
    "                                  reshape_transform=reshape_transform_vit)\n",
    "        grayscale_cam_vit_imagenet = cam_vit_imagenet(input_tensor=img_transformed)\n",
    "        grayscale_cam_vit_imagenet = grayscale_cam_vit_imagenet[0, :]  \n",
    "\n",
    "        img_show = img_transformed.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())  # Normalize to [0,1]\n",
    "\n",
    "        cam_image_vim = show_cam_on_image(img_show, grayscale_cam_vim, use_rgb=True)\n",
    "        cam_image_vim_imagenet = show_cam_on_image(img_show, grayscale_cam_vim_imagenet, use_rgb=True)\n",
    "        cam_image_vit = show_cam_on_image(img_show, grayscale_cam_vit, use_rgb=True)\n",
    "        cam_image_vit_imagenet = show_cam_on_image(img_show, grayscale_cam_vit_imagenet, use_rgb=True)\n",
    "\n",
    "        # Prepare the 2x4 plot\n",
    "        plt.figure(figsize=(35, 20))\n",
    "        gs = gridspec.GridSpec(2, 4)\n",
    "        \n",
    "        # Original image spanning 2x2\n",
    "        ax0 = plt.subplot(gs[0:2, 0:2])\n",
    "        ax0.imshow(img_show)\n",
    "        ax0.set_title('Original Image')\n",
    "        ax0.axis('off')\n",
    "\n",
    "        # Subsequent CAM images\n",
    "        ax1 = plt.subplot(gs[0, 2])\n",
    "        ax1.imshow(cam_image_vim)\n",
    "        ax1.set_title('Grad-CAM-Vim-Cam16', fontsize=40)\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2 = plt.subplot(gs[0, 3])\n",
    "        ax2.imshow(cam_image_vit)\n",
    "        ax2.set_title('Grad-CAM-ViT-Cam16', fontsize=40)\n",
    "        ax2.axis('off')\n",
    "\n",
    "        ax3 = plt.subplot(gs[1, 2])\n",
    "        ax3.imshow(cam_image_vim_imagenet)\n",
    "        ax3.set_title('Grad-CAM-Vim-ImageNet', fontsize=40)\n",
    "        ax3.axis('off')\n",
    "\n",
    "        ax4 = plt.subplot(gs[1, 3])\n",
    "        ax4.imshow(cam_image_vit_imagenet)\n",
    "        ax4.set_title('Grad-CAM-ViT-ImageNet', fontsize=40)\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # Ensure everything is displayed properly\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "        plt.savefig(f'heatmaps/{class_name}/{i}.jpg', bbox_inches='tight', dpi=200)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8524a4a2",
   "metadata": {},
   "source": [
    "## Scaling Vim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ba7b48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDED DIM: 192\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vim-t_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 384\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vim-s_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 384\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vim-s2_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 768\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vit-t_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n",
      "EMBEDDED DIM: 1536\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    }
   ],
   "source": [
    "args.image_size = 224\n",
    "args.patch_size = 16\n",
    "args.num_classes = 2\n",
    "args.n_last_blocks = 4\n",
    "args.avgpool_patchtokens = False\n",
    "\n",
    "args.checkpoint_key = 'teacher'\n",
    "\n",
    "args.arch = 'vim-t'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vim-t_224-96/checkpoint.pth'\n",
    "model_vim_ti = get_model(args)\n",
    "model_vim_ti.cuda()\n",
    "model_vim_ti.eval()\n",
    "load_pretrained_weights(model_vim_ti, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "\n",
    "args.arch = 'vim-s'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vim-s_224-96/checkpoint.pth'\n",
    "model_vim_ti_plus = get_model(args)\n",
    "model_vim_ti_plus.cuda()\n",
    "model_vim_ti_plus.eval()\n",
    "load_pretrained_weights(model_vim_ti_plus, args.pretrained_weights, \n",
    "                        args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "args.arch = 'vim-s2'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vim-s2_224-96/checkpoint.pth'\n",
    "model_vim_s = get_model(args)\n",
    "model_vim_s.cuda()\n",
    "model_vim_s.eval()\n",
    "load_pretrained_weights(model_vim_s, args.pretrained_weights, \n",
    "                        args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "\n",
    "args.arch = 'vit-t'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vit-t_224-96/checkpoint.pth'\n",
    "model_vit_ti = get_model(args)\n",
    "model_vit_ti.cuda()\n",
    "model_vit_ti.eval()\n",
    "load_pretrained_weights(model_vit_ti, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n",
    "args.arch = 'vit-s'\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth'\n",
    "model_vit_s = get_model(args)\n",
    "model_vit_s.cuda()\n",
    "model_vit_s.eval()\n",
    "load_pretrained_weights(model_vit_s, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bbc3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 120/120 [01:02<00:00,  1.91it/s]\n",
      "100%|█████████████████████████████████████████| 120/120 [01:05<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for class_name in ['tumor', 'normal']:\n",
    "\n",
    "    img_paths = glob(os.path.join(dataset_dir, class_name, \"*jpg\"))\n",
    "    img_paths = os_sorted(img_paths)\n",
    "\n",
    "    os.makedirs(f'heatmaps_scaling/{class_name}', exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(120)):\n",
    "        img = Image.open(img_paths[i])\n",
    "        img_transformed = val_transform(img).unsqueeze(0)\n",
    "        img_show = img_transformed.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())  # Normalize to [0,1]\n",
    "\n",
    "        plt.figure(figsize=(35, 20))\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(img_show)\n",
    "        plt.title('Original Image', fontsize=40)\n",
    "        plt.axis('off')\n",
    "\n",
    "        for j, (model_name, model_vim) in enumerate([('Vim-ti', model_vim_ti), \n",
    "                                      ('Vim-ti-plus', model_vim_ti_plus), ('Vim-s', model_vim_s)]):\n",
    "            cam_vim = GradCAM(model=model_vim, target_layers=[model_vim.layers[-1].drop_path],\n",
    "                              reshape_transform=reshape_transform_vim)\n",
    "            grayscale_cam_vim = cam_vim(input_tensor=img_transformed)\n",
    "            grayscale_cam_vim = grayscale_cam_vim[0, :]  \n",
    "\n",
    "\n",
    "            cam_image_vim = show_cam_on_image(img_show, grayscale_cam_vim, use_rgb=True)\n",
    "        \n",
    "\n",
    "            # Subsequent CAM images\n",
    "            plt.subplot(1, 4, j+2)\n",
    "            plt.imshow(cam_image_vim)\n",
    "            plt.title(model_name, fontsize=40)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Ensure everything is displayed properly\n",
    "            plt.tight_layout()\n",
    "\n",
    "\n",
    "        plt.savefig(f'heatmaps_scaling/{class_name}/{i}.jpg', bbox_inches='tight', dpi=200)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f58b",
   "metadata": {},
   "source": [
    "# All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "import h5py\n",
    "from shapely.affinity import scale\n",
    "from shapely.geometry import Polygon, box, mapping\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_correct_polygon(polygon):\n",
    "    if not polygon.is_valid:\n",
    "        corrected_polygon = polygon.buffer(0)\n",
    "        if corrected_polygon.is_valid:\n",
    "            return corrected_polygon\n",
    "        else:\n",
    "            # Further correction attempts or logging for manual review\n",
    "            # For complex cases, consider using simplify() with a small tolerance\n",
    "            simplified_polygon = polygon.simplify(1.0, preserve_topology=True)\n",
    "            if simplified_polygon.is_valid:\n",
    "                return simplified_polygon\n",
    "            else:\n",
    "                raise ValueError(\"Polygon could not be corrected and remains invalid.\")\n",
    "    return polygon\n",
    "\n",
    "def xml_to_polygons(xml_path):\n",
    "    tree = etree.parse(xml_path)\n",
    "    annotations = []\n",
    "    for annotation in tree.xpath('.//Annotation'):\n",
    "        points = []\n",
    "        for coordinate in annotation.xpath('.//Coordinate'):\n",
    "            x = float(coordinate.get('X'))\n",
    "            y = float(coordinate.get('Y'))\n",
    "            points.append([x, y])\n",
    "        annotations.append(points)\n",
    "    \n",
    "    polygons = []\n",
    "    for annotation in annotations:\n",
    "        if len(annotation)>3:\n",
    "            polygon = Polygon(annotation)\n",
    "            corrected_polygon = validate_and_correct_polygon(polygon)\n",
    "            polygons.append(corrected_polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def polygons_to_patches(polygons, slide_dimension, patch_level, overlap = 0.1):    \n",
    "    step_size = int(slide_dimension * (1 - overlap))*(2**patch_level)\n",
    "    patch_area = slide_dimension ** 2  \n",
    "    patches_info = []\n",
    "    for target_polygon in polygons:\n",
    "        min_x, min_y, max_x, max_y = map(int, target_polygon.bounds)\n",
    "        for x_start in np.arange(min_x, max_x - slide_dimension + 1, step_size):\n",
    "            for y_start in np.arange(min_y, max_y - slide_dimension + 1, step_size):\n",
    "                x_end = int(x_start) + slide_dimension\n",
    "                y_end = int(y_start) + slide_dimension\n",
    "\n",
    "                # Create a shapely Polygon object for the current patch\n",
    "                patch_polygon = box(x_start, y_start, x_end, y_end)\n",
    "\n",
    "                # Calculate the intersection of the patch with the target polygon\n",
    "                intersection = target_polygon.intersection(patch_polygon)\n",
    "\n",
    "                # Calculate the percentage of the patch that lies within the target polygon\n",
    "                percentage_inside = (intersection.area / patch_area) * 100\n",
    "\n",
    "                patches_info.append({\n",
    "                    'coordinates': (x_start, y_start),\n",
    "                    'percentage_inside': percentage_inside,\n",
    "                    'target_polygon': target_polygon,\n",
    "                    'intersection':intersection\n",
    "                })\n",
    "    return patches_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea17e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_res_img(img_path):\n",
    "    \n",
    "    file_name = img_path.split('/')[-1].split('.')[0]\n",
    "    wsi_name = '_'.join(file_name.split('_')[:-1])\n",
    "    tumor_idx = int(file_name.split('_')[-1])\n",
    "\n",
    "    wsi = openslide.open_slide(f'/home/ubuntu/Downloads/Camelyon16/testing/images/{wsi_name}.tif')\n",
    "    hdf5_file = h5py.File(f'dataset/Camelyon16/testing/224_10x/h5/images/patches/{wsi_name}.h5', 'r')\n",
    "\n",
    "    patch_level = hdf5_file['coords'].attrs['patch_level']\n",
    "    patch_size = hdf5_file['coords'].attrs['patch_size']\n",
    "    polygons = xml_to_polygons(f'/home/ubuntu/Downloads/Camelyon16/testing/lesion_annotations/{wsi_name}.xml')\n",
    "    patches_info = polygons_to_patches(polygons, slide_dimension=patch_size, patch_level=patch_level)\n",
    "    hdf5_file.close()\n",
    "    idx = 0\n",
    "    for i in range(len(patches_info)):\n",
    "        if patches_info[i]['percentage_inside']>50:\n",
    "            idx += 1 \n",
    "        if idx==tumor_idx+1:\n",
    "            break\n",
    "    \n",
    "    coord = patches_info[i]['coordinates']\n",
    "    img = wsi.read_region((coord[0], coord[1]), patch_level//4, (patch_size*4, patch_size*4)).convert('RGB') \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b9964a",
   "metadata": {},
   "source": [
    "First slide only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7126d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [04:05<00:00,  4.09s/it]\n",
      "100%|███████████████████████████████████████████| 60/60 [01:47<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Vim-ti':(model_vim_ti, model_vim_ti.layers[-1].drop_path),\n",
    "    'Vim-ti-plus':(model_vim_ti_plus, model_vim_ti_plus.layers[-1].drop_path),\n",
    "    'Vim-s':(model_vim_s, model_vim_s.layers[-1].drop_path),\n",
    "    'ViT-ti':(model_vit_ti, model_vit_ti.blocks[-1].norm1),\n",
    "    'ViT-s':(model_vit_s, model_vit_s.blocks[-1].norm1)\n",
    "}\n",
    "\n",
    "\n",
    "for class_name in ['tumor', 'normal']:\n",
    "\n",
    "    img_paths = glob(os.path.join(dataset_dir, class_name, \"*jpg\"))\n",
    "    img_paths = os_sorted(img_paths)\n",
    "\n",
    "    os.makedirs(f'heatmaps_single/{class_name}', exist_ok=True)\n",
    "    os.makedirs(f'heatmaps_single/{class_name}/raw', exist_ok=True)\n",
    "    for i in tqdm(range(60)):\n",
    "        img = Image.open(img_paths[i])\n",
    "        img_transformed = val_transform(img).unsqueeze(0)\n",
    "        img_show = img_transformed.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())  # Normalize to [0,1]\n",
    "        plt.figure(figsize=(50, 23))\n",
    "        gs = gridspec.GridSpec(2, 5)\n",
    "        \n",
    "        # Original image\n",
    "        if class_name == 'tumor':\n",
    "            final_img = get_high_res_img(img_paths[i])\n",
    "        else:\n",
    "            final_img = img\n",
    "        img_name = os.path.splitext(os.path.basename(img_paths[i]))[0]\n",
    "        final_img.save(f'heatmaps_single/{class_name}/raw/{img_name}_orig.png')\n",
    "        \n",
    "        final_img = np.array(final_img)/255\n",
    "            \n",
    "        ax0 = plt.subplot(gs[0:2, 0:2])\n",
    "        ax0.imshow(final_img)\n",
    "        ax0.set_title('Original Image', fontsize=40)\n",
    "        ax0.axis('off')\n",
    "\n",
    "        \n",
    "        cams = []\n",
    "        for idx, (model_name, (model, target_layer)) in enumerate(models.items()):\n",
    "            cam = GradCAM(model=model, target_layers=[target_layer], \n",
    "                          reshape_transform=reshape_transform_vim if 'mamba' in model.__class__.__name__.lower() else reshape_transform_vit)\n",
    "            grayscale_cam = cam(input_tensor=img_transformed)[0, :]\n",
    "            grayscale_cam = cv2.resize(grayscale_cam, (final_img.shape[:2]))\n",
    "            cam_image = show_cam_on_image(final_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "            ax = plt.subplot(gs[idx // 3, idx % 3 + 2])\n",
    "            ax.imshow(cam_image)\n",
    "            Image.fromarray(cam_image).save(f'heatmaps_all/{class_name}/raw/{img_name}_{model_name}.png')\n",
    "\n",
    "            ax.set_title(f'{model_name} Heatmap', fontsize=40)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'heatmaps_single/{class_name}/{img_name}.jpg', bbox_inches='tight', dpi=200)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede07c6",
   "metadata": {},
   "source": [
    "Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7e25edca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [09:53<00:00,  9.89s/it]\n",
      "100%|███████████████████████████████████████████| 60/60 [02:18<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Vim-ti':(model_vim_ti, model_vim_ti.layers[-1].drop_path),\n",
    "    'Vim-ti-plus':(model_vim_ti_plus, model_vim_ti_plus.layers[-1].drop_path),\n",
    "    'Vim-s':(model_vim_s, model_vim_s.layers[-1].drop_path),\n",
    "    'ViT-ti':(model_vit_ti, model_vit_ti.blocks[-1].norm1),\n",
    "    'ViT-s':(model_vit_s, model_vit_s.blocks[-1].norm1)\n",
    "}\n",
    "\n",
    "for class_name in ['tumor', 'normal']:\n",
    "\n",
    "    img_paths = glob(os.path.join(dataset_dir, class_name, \"*jpg\"))\n",
    "    img_paths = os_sorted(img_paths)\n",
    "    target_image_idx = list(np.random.randint(0, len(img_paths), 60))\n",
    "\n",
    "    os.makedirs(f'heatmaps_diverse/{class_name}', exist_ok=True)\n",
    "    os.makedirs(f'heatmaps_diverse/{class_name}/raw', exist_ok=True)\n",
    "\n",
    "    for i in tqdm(target_image_idx):\n",
    "        img = Image.open(img_paths[i])\n",
    "        img_transformed = val_transform(img).unsqueeze(0)\n",
    "        img_show = img_transformed.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())  # Normalize to [0,1]\n",
    "        plt.figure(figsize=(50, 23))\n",
    "        gs = gridspec.GridSpec(2, 5)\n",
    "        \n",
    "        # Original image\n",
    "        if class_name == 'tumor':\n",
    "            final_img = get_high_res_img(img_paths[i])\n",
    "        else:\n",
    "            final_img = img\n",
    "        img_name = os.path.splitext(os.path.basename(img_paths[i]))[0]\n",
    "        final_img.save(f'heatmaps_diverse/{class_name}/raw/{img_name}_orig.png')\n",
    "        \n",
    "        final_img = np.array(final_img)/255\n",
    "            \n",
    "        ax0 = plt.subplot(gs[0:2, 0:2])\n",
    "        ax0.imshow(final_img)\n",
    "        ax0.set_title('Original Image', fontsize=40)\n",
    "        ax0.axis('off')\n",
    "\n",
    "        \n",
    "        cams = []\n",
    "        for idx, (model_name, (model, target_layer)) in enumerate(models.items()):\n",
    "            cam = GradCAM(model=model, target_layers=[target_layer], \n",
    "                          reshape_transform=reshape_transform_vim if 'mamba' in model.__class__.__name__.lower() else reshape_transform_vit)\n",
    "            grayscale_cam = cam(input_tensor=img_transformed)[0, :]\n",
    "            grayscale_cam = cv2.resize(grayscale_cam, (final_img.shape[:2]))\n",
    "            cam_image = show_cam_on_image(final_img, grayscale_cam, use_rgb=True)\n",
    "\n",
    "            ax = plt.subplot(gs[idx // 3, idx % 3 + 2])\n",
    "            ax.imshow(cam_image)\n",
    "            Image.fromarray(cam_image).save(f'heatmaps_diverse/{class_name}/raw/{img_name}_{model_name}.png')\n",
    "            ax.set_title(f'{model_name} Heatmap', fontsize=40)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'heatmaps_diverse/{class_name}/{img_name}.jpg', bbox_inches='tight', dpi=200)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c529e",
   "metadata": {},
   "source": [
    "##### Attention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_compute_attention_heatmap(model, img, device, threshold=None):\n",
    "    w_featmap = img.shape[-2] // args.patch_size\n",
    "    h_featmap = img.shape[-1] // args.patch_size\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "    if threshold is not None:\n",
    "        # we keep only a certain percentage of the mass\n",
    "        val, idx = torch.sort(attentions)\n",
    "        val /= torch.sum(val, dim=1, keepdim=True)\n",
    "        cumval = torch.cumsum(val, dim=1)\n",
    "        th_attn = cumval > (1 - threshold)\n",
    "        idx2 = torch.argsort(idx)\n",
    "        for head in range(nh):\n",
    "            th_attn[head] = th_attn[head][idx2[head]]\n",
    "        th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "        # interpolate\n",
    "        th_attn = nn.functional.interpolate(th_attn.unsqueeze(0),\n",
    "                                            scale_factor=args.patch_size, mode=\"nearest\")[0].cpu().detach().numpy()\n",
    "\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), \n",
    "                                           scale_factor=args.patch_size, mode=\"nearest\")[0].cpu().detach().numpy()\n",
    "    attentions = np.repeat(attentions[:, :, :, np.newaxis], 3, -1)\n",
    "    attentions = (attentions-attentions.min())/attentions.max()\n",
    "    attentions.max()\n",
    "\n",
    "    img = torchvision.utils.make_grid(img, normalize=True, scale_each=True)\n",
    "    return img, attentions\n",
    "\n",
    "img = Image.open(img_paths[40])\n",
    "img = val_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "device = 'cuda'\n",
    "img, attentions = vit_compute_heatmap(model_vit, img, device)\n",
    "img = img.transpose(0, 1).transpose(1, 2).cpu().numpy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(np.concatenate([img, attentions.max(0)], axis=1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab464d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vim",
   "language": "python",
   "name": "vim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
