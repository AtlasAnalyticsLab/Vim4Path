{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c162c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dino.vision_transformer import DINOHead, VisionTransformer\n",
    "from dino.vim.models_mamba import VisionMamba\n",
    "from dino.config import configurations\n",
    "from dino.main import get_args_parser\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from dino.utils import load_pretrained_weights\n",
    "\n",
    "from MIL.models.resnet_custom import resnet50_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04463ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_args_parser()\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e755146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDED DIM: 1536\n",
      "Take key teacher in provided checkpoint dict\n",
      "Skipping loading parameter head.weight due to size mismatch or it not being present in the checkpoint.\n",
      "Skipping loading parameter head.bias due to size mismatch or it not being present in the checkpoint.\n",
      "Pretrained weights found at /home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])\n"
     ]
    }
   ],
   "source": [
    "args.arch = 'vit-s'\n",
    "args.image_size = 224\n",
    "args.patch_size = 16\n",
    "args.num_classes = 2\n",
    "args.n_last_blocks = 4\n",
    "args.avgpool_patchtokens = False\n",
    "\n",
    "if args.arch in configurations:\n",
    "    config = configurations[args.arch]\n",
    "    config['img_size'] = args.image_size\n",
    "    config['patch_size'] = args.patch_size\n",
    "    config['num_classes'] = args.num_classes\n",
    "\n",
    "    if 'norm_layer' in config and config['norm_layer'] == \"nn.LayerNorm\":\n",
    "        config['norm_layer'] = partial(nn.LayerNorm, eps=config['eps'])\n",
    "    config['drop_path_rate'] = 0  \n",
    "    if args.arch.startswith('vim'):\n",
    "        config['final_pool_type']='all'\n",
    "        model = VisionMamba(return_features=True, **config)\n",
    "        embed_dim = model.embed_dim\n",
    "    elif args.arch.startswith('vit'):\n",
    "        model = VisionTransformer(**config)\n",
    "        embed_dim = model.embed_dim * (args.n_last_blocks + int(args.avgpool_patchtokens))\n",
    "    print('EMBEDDED DIM:', embed_dim)\n",
    "elif args.arch=='resnet':\n",
    "    model = resnet50_baseline(pretrained=True)\n",
    "else:\n",
    "    print(f\"Unknown architecture: {args.arch}\")\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "args.pretrained_weights = '/home/ubuntu/checkpoints/camelyon16_224_10x/vit-s_224-96/checkpoint.pth'\n",
    "args.checkpoint_key = 'teacher'\n",
    "load_pretrained_weights(model, args.pretrained_weights, args.checkpoint_key, args.arch, args.patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a0c0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from natsort import os_sorted\n",
    "import time\n",
    "import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dabce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feaures(model, inp):\n",
    "    with torch.no_grad():\n",
    "        if \"vit\" in args.arch:\n",
    "            intermediate_output = model.get_intermediate_layers(inp, args.n_last_blocks)\n",
    "            output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "            if args.avgpool_patchtokens:\n",
    "                output = torch.cat((output.unsqueeze(-1), \n",
    "                                    torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "                output = output.reshape(output.shape[0], -1)\n",
    "        else:\n",
    "            output = model(inp)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e566dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_w_loader(file_path, output_path, wsi, model,\n",
    "    batch_size = 8, verbose = 0, print_every=20, pretrained=True, \n",
    "    custom_downsample=1, target_patch_size=-1):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        file_path: directory of bag (.h5 file)\n",
    "        output_path: directory to save computed features (.h5 file)\n",
    "        model: pytorch model\n",
    "        batch_size: batch_size for computing features in batches\n",
    "        verbose: level of feedback\n",
    "        pretrained: use weights pretrained on imagenet\n",
    "        custom_downsample: custom defined downscale factor of image patches\n",
    "        target_patch_size: custom defined, rescaled image size before embedding\n",
    "    \"\"\"\n",
    "    dataset = Whole_Slide_Bag_FP(file_path=file_path, wsi=wsi, pretrained=pretrained, \n",
    "        custom_downsample=custom_downsample, target_patch_size=target_patch_size)\n",
    "    x, y = dataset[0]\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if device.type == \"cuda\" else {}\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, **kwargs, collate_fn=collate_features)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print('processing {}: total of {} batches'.format(file_path,len(loader)))\n",
    "\n",
    "    mode = 'w'\n",
    "    for count, (batch, coords) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "#             if count % print_every == 0:\n",
    "#                 print('batch {}/{}, {} files processed'.format(count, len(loader), count * batch_size))\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "\n",
    "            features = get_feaures(model, batch)\n",
    "            features = features.cpu().numpy()\n",
    "\n",
    "            asset_dict = {'features': features, 'coords': coords}\n",
    "            save_hdf5(output_path, asset_dict, attr_dict= None, mode=mode)\n",
    "            mode = 'a'\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bda7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MIL.datasets.dataset_h5 import Dataset_All_Bags, Whole_Slide_Bag_FP\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from MIL.utils.utils import print_network, collate_features\n",
    "from MIL.utils.file_utils import save_hdf5\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34aeefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_split(feat_dir, classes, slide_folder, h5_dir):\n",
    "    os.makedirs(os.path.join(feat_dir, 'pt_files'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(feat_dir, 'h5_files'), exist_ok=True)\n",
    "    dest_files = os.listdir(os.path.join(feat_dir, 'pt_files'))\n",
    "\n",
    "    for class_name in classes:\n",
    "        print(f\"Processing for class {class_name}\")\n",
    "        args.data_h5_dir = os.path.join(h5_dir, class_name)\n",
    "        slide_paths = glob.glob(os.path.join(slide_folder, class_name, '*tif'))\n",
    "        for slide_file_path in tqdm(slide_paths):\n",
    "            slide_id = os.path.splitext(os.path.basename(slide_file_path))[0]\n",
    "#             if os.path.exists(os.path.join(feat_dir, 'pt_files', slide_id+'.pt')):\n",
    "#                 continue\n",
    "            bag_name = slide_id+'.h5'\n",
    "            h5_file_path = os.path.join(args.data_h5_dir, 'patches', bag_name)\n",
    "            output_path = os.path.join(feat_dir, 'h5_files', bag_name)\n",
    "            wsi = openslide.open_slide(slide_file_path)\n",
    "            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            args.batch_size=16\n",
    "            output_file_path = compute_w_loader(h5_file_path, output_path, wsi, \n",
    "                                                model = model, batch_size = args.batch_size, verbose = 0, \n",
    "                                                print_every = 20, custom_downsample=1, \n",
    "                                                target_patch_size=args.image_size)\n",
    "\n",
    "\n",
    "            with h5py.File(output_file_path, \"r\") as file:\n",
    "                features = file['features'][:]\n",
    "                features = torch.from_numpy(features)\n",
    "                torch.save(features, os.path.join(feat_dir, 'pt_files', slide_id+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbec668",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(features, os.path.join(feat_dir, 'pt_files', slide_id+'.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f3e8e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: training\n",
      "Processing for class normal\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b7143a63624f53a6788e633684bbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for class tumor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a2b8be4d204ab19a154904926b058e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = ['normal', 'tumor']\n",
    "output_dir = 'clam_data/'\n",
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "split = 'training'\n",
    "print(f\"Processing split: {split}\")\n",
    "slide_folder = f'/home/ubuntu/Downloads/Camelyon16/{split}/'\n",
    "feat_dir = f'{output_dir}/{args.image_size}_10x/{args.arch}/{split}'\n",
    "h5_dir = f'dataset/Camelyon16/{split}/{args.image_size}_10x/h5/'\n",
    "\n",
    "create_data_split(feat_dir, classes, slide_folder, h5_dir)\n",
    "\n",
    "# create csv\n",
    "\n",
    "slide_ids = os_sorted(os.listdir(os.path.join(feat_dir, 'h5_files')))\n",
    "slide_ids = [i.split('.')[0] for i in slide_ids ]\n",
    "label = [i.split('_')[0] for i in slide_ids ]\n",
    "df = pd.DataFrame([slide_ids, slide_ids, label]).T\n",
    "df.columns = ['case_id', 'slide_id', 'label']\n",
    "df.to_csv(os.path.join(feat_dir, 'tumor_vs_normal.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e30c1694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: testing\n",
      "Processing for class images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df44c49ec9947958054fff1bc295f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = 'clam_data/'\n",
    "device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n",
    "split = 'testing'\n",
    "print(f\"Processing split: {split}\")\n",
    "slide_folder = f'/home/ubuntu/Downloads/Camelyon16/{split}/'\n",
    "feat_dir = f'{output_dir}/{args.image_size}_10x/{args.arch}/{split}'\n",
    "h5_dir = f'dataset/Camelyon16/{split}/{args.image_size}_10x/h5/'\n",
    "\n",
    "classes = ['images']\n",
    "create_data_split(feat_dir, classes, slide_folder, h5_dir)\n",
    "\n",
    "# create csv\n",
    "df = pd.read_csv('/home/ubuntu/Downloads/Camelyon16/testing/reference.csv', header=None)\n",
    "df = df.drop([2, 3], axis=1)\n",
    "df[2] = df[0]\n",
    "df = df[[0, 2, 1]]\n",
    "df.columns = ['case_id', 'slide_id', 'label']\n",
    "df['label'] = df['label'].apply(str.lower)\n",
    "df.to_csv(os.path.join(feat_dir, 'tumor_vs_normal.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1c364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vim",
   "language": "python",
   "name": "vim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
